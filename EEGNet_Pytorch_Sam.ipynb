{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EEGNet-Pytorch-Sam.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMTOdFGmXQiYCXWbkBTriCV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smichalka/premusings/blob/master/EEGNet_Pytorch_Sam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lV9AOTQyOk5G",
        "colab_type": "text"
      },
      "source": [
        "# Summary\n",
        "\n",
        "This code builds upon EEGNet (originally written using Keras here: https://github.com/vlawhern/arl-eegmodels/blob/master/EEGModels.py. The folder also includes links to some great papers and has excellent descriptions of their parameters). Here we are implementing in PyTorch. The starting parameters are actually derived from EEGNet_SSVEP. We will be applying this to our data, which uses an SSVEP paradigm.\n",
        "\n",
        "Details of our data:\n",
        "Participants attended to video with an overlayed flashing checkerboard at 12 or 15 Hz on the left side of a computer screen. The right side showed another video with a checkerboard flashing at the other frequency. Our goal here is to classify if a person was attending to the 12 or 15 Hz. \n",
        "\n",
        "This code was written by KF, Sam Michalka, and AD.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtaNIg0AFZD8",
        "colab_type": "text"
      },
      "source": [
        "## Things to do next\n",
        "\n",
        "\n",
        "*   Try downsampling the time data to something closer to 100 hz or 125 hz, potentially using average pooling across the time dimension (if this is appropriate)\n",
        "* Fix the convolutions to make sure they are depthwise and pointwise, need to use groups, might need another conv\n",
        "* Add in padding because right now your layers are shrinking in time dimension through convolution and you have to hard-code the sizes in. This might be preventable if you can get the temporal/bandpass filter to be an odd number, which is somewhat dependent on the sampling rate... so maybe 250 is good or 150 (but that's an uneven break into 500).\n",
        "* add in plots to visualize the layers. See https://mc.ai/feature-visualisation-in-pytorch%E2%80%8A-%E2%80%8Asaliency-maps/ for ideas\n",
        "\n",
        "\n",
        "\n",
        "* figure out how to set up cross validation, or determine this might not be appropriate (if the model takes too long to load), in which case you should still do a proper train/val split of the data. See https://skorch.readthedocs.io/en/stable/user/dataset.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5gG2Gd1QB0h",
        "colab_type": "text"
      },
      "source": [
        "# Setup, imports, etc\n",
        "\n",
        "## Setup Notes\n",
        "\n",
        "\n",
        "*   Set hardware accelerator to GPU (runtime->change runtime type->change hardware accelerator to GPU), otherwise you may get an 'CUDA-capable device' error \n",
        "\n",
        "*   Will need to connect to Google Drive to import data (or load another way)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLtx8Wr-P_mx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "#from torch.autograd import Variable\n",
        "#import torch.nn.functional as F\n",
        "#import torch.optim as optim\n",
        "#from sklearn.metrics import roc_auc_score, precision_score, recall_score, accuracy_score\n",
        "\n",
        "!pip install torchviz\n",
        "from torchviz import make_dot\n",
        "from torch.autograd import Variable\n",
        "from torch.utils import data\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "from torchvision import models\n",
        "from torchsummary import summary #Allows you to look at the sizes of your layers\n",
        "\n",
        "# Can skip these ones if just building with fake data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive') # allow access to google drive as if it were a local filesystem\n",
        "# Should probably just mount this folder not all of drive\n",
        "\n",
        "# CUDA for PyTorch\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "cudnn.benchmark = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2kvieqgRN_6",
        "colab_type": "text"
      },
      "source": [
        "# Data format and loading\n",
        "The data is shaped as a (1, Channels, Samples or Timepoints).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7Dr_ygEQBB2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate fake data to start\n",
        "X_train = np.random.rand(32*4,1,32,1499).astype('float32') # np.random.rand generates between [0, 1)\n",
        "y_train = np.round(np.random.rand(32*4).astype('float32')) # binary data, so we round it to 0 or 1.\n",
        "# # y is conditions, so either 0 or 1\n",
        "\n",
        "train_sampler = SubsetRandomSampler(\n",
        "    np.arange(X_train.shape[0], dtype=np.int64))\n",
        "\n",
        "# This part will be need to be repeated when you load real data \n",
        "srate = 500 #sampling rate in Hz\n",
        "data_dims = X_train.shape[1:] #First dimension is number of trials/examples\n",
        "print(data_dims)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7KGe7RsNiul",
        "colab_type": "text"
      },
      "source": [
        "Load data based on Katie's code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wH0RcuDuNB7U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "subj = \"BN\"\n",
        "\n",
        "# Load the conditions\n",
        "conditions = np.loadtxt(open(\"/content/drive/My Drive/HAL/Projects/EEGvideodataforMachineLearning/BN/train/conditions.csv\", \"rb\"), delimiter=\",\", skiprows=1, dtype=int)\n",
        "np.shape(conditions)[0]\n",
        "y = conditions[:,1]%2  # Even conditions are 0 (high freq 15) and odd are 1 (low freq 12)\n",
        "y= y.astype(np.float32)\n",
        "print(\"Shape of y\", y.shape)\n",
        "\n",
        "# Load the raw data (X)\n",
        "X = np.array([])\n",
        "X = np.loadtxt(open(\"/content/drive/My Drive/HAL/Projects/EEGvideodataforMachineLearning/\"+subj+\"/train/1.csv\", \"rb\"), delimiter=\",\", skiprows=1)\n",
        "for i in range (1, y.shape[0]): # 531\n",
        "    filepath = \"/content/drive/My Drive/HAL/Projects/EEGvideodataforMachineLearning/\"+subj+\"/train/\"+str(i+1)+\".csv\"\n",
        "    # load data from filepath\n",
        "    if (i-1)%100 == 0:\n",
        "      print(\"Downloading data from:\", filepath) \n",
        "    data = np.loadtxt(open(filepath, \"rb\"), delimiter=\",\", skiprows=1) \n",
        "    X = np.dstack((X, data))\n",
        "print(np.shape(X))\n",
        "\n",
        "# Rearrange to have number of trials first, then an empty dimension (channels for image, not ch for eeg)\n",
        "X = np.moveaxis(X, -1,0)\n",
        "print(\"Shape of X\", np.shape(X))\n",
        "\n",
        "X = np.expand_dims(X, axis=1)\n",
        "print(np.shape(X)) # aiming for: (531, 1, 1499, 32)\n",
        "X = X.astype(np.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcg5aDp6c0w_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now make these Tensors and make them a dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTZ7BrG_TRxu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Divide up into train and val\n",
        "\n",
        "tvboundary = 400\n",
        "\n",
        "y_train = y[:tvboundary]\n",
        "X_train = X[:tvboundary]\n",
        "print(y_train.shape)\n",
        "\n",
        "y_val = y[tvboundary:]\n",
        "X_val = X[tvboundary:]\n",
        "print(y_val.shape)\n",
        "\n",
        "data_dims = X_train.shape[1:] #First dimension is number of trials/examples\n",
        "print(data_dims)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wv8zoTV2Sl3E",
        "colab_type": "text"
      },
      "source": [
        "#EEGNet_SSVEP Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cINqAl7ZSq8N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EEGNet_SSVEP(nn.Module):\n",
        "  \"\"\" Pytorch Implementation of EEGNet\n",
        "    http://iopscience.iop.org/article/10.1088/1741-2552/aace8c/meta\n",
        "    Note that this implements the newest version of EEGNet and NOT the earlier\n",
        "    version (version v1 and v2 on arxiv). For example:\n",
        "        \n",
        "        1. Depthwise Convolutions to learn spatial filters within a \n",
        "        temporal convolution. The use of the depth_multiplier option maps \n",
        "        exactly to the number of spatial filters learned within a temporal\n",
        "        filter. This matches the setup of algorithms like FBCSP which learn \n",
        "        spatial filters within each filter in a filter-bank. This also limits \n",
        "        the number of free parameters to fit when compared to a fully-connected\n",
        "        convolution. \n",
        "        \n",
        "        2. Separable Convolutions to learn how to optimally combine spatial\n",
        "        filters across temporal bands. Separable Convolutions are Depthwise\n",
        "        Convolutions followed by (1x1) Pointwise Convolutions. \n",
        "        \n",
        "    \n",
        "    While the original paper used Dropout, we found that SpatialDropout2D \n",
        "    sometimes produced slightly better results for classification of ERP \n",
        "    signals. However, SpatialDropout2D significantly reduced performance \n",
        "    on the Oscillatory dataset (SMR, BCI-IV Dataset 2A). We recommend using\n",
        "    the default Dropout in most cases.\n",
        "        \n",
        "    Assumes the input signal is sampled at 128Hz. If you want to use this model\n",
        "    for any other sampling rate you will need to modify the lengths of temporal\n",
        "    kernels and average pooling size in blocks 1 and 2 as needed (double the \n",
        "    kernel lengths for double the sampling rate, etc). Note that we haven't \n",
        "    tested the model performance with this rule so this may not work well. \n",
        "    \n",
        "    The model with default parameters gives the EEGNet-8,2 model as discussed\n",
        "    in the paper. This model should do pretty well in general, although it is\n",
        "\tadvised to do some model searching to get optimal performance on your\n",
        "\tparticular dataset.\n",
        "    We set F2 = F1 * D (number of input filters = number of output filters) for\n",
        "    the SeparableConv2D layer. We haven't extensively tested other values of this\n",
        "    parameter (say, F2 < F1 * D for compressed learning, and F2 > F1 * D for\n",
        "    overcomplete). We believe the main parameters to focus on are F1 and D. \n",
        "    Inputs:\n",
        "        \n",
        "      nb_classes      : int, number of classes to classify\n",
        "      Chans, Samples  : number of channels and time points in the EEG data\n",
        "      dropoutRate     : dropout fraction\n",
        "      kernLength      : length of temporal convolution in first layer. We found\n",
        "                        that setting this to be half the sampling rate worked\n",
        "                        well in practice. For the SMR dataset in particular\n",
        "                        since the data was high-passed at 4Hz we used a kernel\n",
        "                        length of 32.     \n",
        "      F1, F2          : number of temporal filters (F1) and number of pointwise\n",
        "                        filters (F2) to learn. Default: F1 = 8, F2 = F1 * D. \n",
        "      D               : number of spatial filters to learn within each temporal\n",
        "                        convolution. Default: D = 2\n",
        "      dropoutType     : Either SpatialDropout2D or Dropout, passed as a string.\n",
        "      \n",
        "      [1]. Waytowich, N. et. al. (2018). Compact Convolutional Neural Networks\n",
        "    for Classification of Asynchronous Steady-State Visual Evoked Potentials.\n",
        "    Journal of Neural Engineering vol. 15(6). \n",
        "    http://iopscience.iop.org/article/10.1088/1741-2552/aae5d8\n",
        "    \"\"\"\n",
        "  def __init__(self):\n",
        "    super(EEGNet_SSVEP,self).__init__()\n",
        "\n",
        "    # Settings (can we potentially make this a function with input params?)\n",
        "    nb_classes = 2\n",
        "    Chans = data_dims[1]\n",
        "    Samples = data_dims[2]\n",
        "    kernLength = round(srate/2) #should be 250 for 500 Hz  #Actually want this to be an odd number to avoid padding issues, so may need to revisit or add padding manually\n",
        "    print(\"Kernel length: \", kernLength)\n",
        "    kernSepLength = 16\n",
        "    F1 = 24\n",
        "    F2 = 96  # must be divisible by F1*D?\n",
        "    D = 2\n",
        "\n",
        "    Favgpool1 = 4\n",
        "    Favgpool2 = 8\n",
        "    dropout_rate = .5  # might need to explore making this in one direction only?\n",
        "    fc1_size = 10 # number of classes right now, might want to have another layer\n",
        "\n",
        "    # BLOCK 1\n",
        "\n",
        "\n",
        "    #self.avgpool1 = nn.AvgPool2d((1,Favgpool1))\n",
        "    # Convolution in time (essentially band pass)\n",
        "    self.convtime = nn.Conv2d(data_dims[0],F1,(1,kernLength),padding=0,padding_mode='replicate',bias=False)\n",
        "    \n",
        "    # Batch normalization (was axis=1 in keras, not sure equiv) \n",
        "    self.batchnorm1 = nn.BatchNorm2d(F1, eps=1e-05)\n",
        "    \n",
        "    # Depthwise convolution \n",
        "    self.convdepth = nn.Conv2d(F1,F1*D,(Chans,1),padding=0,padding_mode='replicate',bias=False,groups = F1) \n",
        "    # See https://pytorch.org/docs/master/generated/torch.nn.Conv2d.html#torch.nn.Conv2d for making this depthwise\n",
        "    #If groups = nInputPlane, then it is Depthwise. \n",
        "    #If groups = nInputPlane, kernel=(K, 1), (and before is a Conv2d layer with groups=1 and kernel=(1, K)), then it is separable.\n",
        "    \n",
        "    # Batch normalization (was axis=1 in keras, not sure equiv) \n",
        "    self.batchnorm2 = nn.BatchNorm2d(F1*D, eps=1e-05)\n",
        "    \n",
        "    # Activation function internally in the network\n",
        "    self.activation_func = torch.nn.ReLU()\n",
        "\n",
        "    # Average pooling along the time dimension (downsampling)\n",
        "    self.avgpool1 = nn.AvgPool2d((1,Favgpool1))\n",
        "    # Followed by dropout\n",
        "\n",
        "    # BLOCK 2\n",
        "    self.convsep = nn.Conv2d(F1*D,F2,(1,kernSepLength), padding=0,padding_mode='replicate',bias=False, groups=F1*D)  #need bias and padding info\n",
        "    # might need a second convolution here\n",
        "\n",
        "    # Batch normalization (was axis=1 in keras, not sure equiv) \n",
        "    self.batchnorm3 = nn.BatchNorm2d(F2, eps=1e-05)\n",
        "    \n",
        "    # Need another round of conv, norm, activation, average  pooling\n",
        "    self.avgpool2 = nn.AvgPool2d((1,Favgpool2))\n",
        "\n",
        "    # Then flatten to linear\n",
        "\n",
        "    # Fully connected layer - note the change in inputs \n",
        "    #self.output_size_2 = int(Chans*F2)\n",
        "    self.fc1 = nn.Linear(37*F2, fc1_size)\n",
        "    \n",
        "    # Final fc layer to output. This is binary right now, but could also be number of classes, then you need to change the labels and loss\n",
        "    self.fc2 = nn.Linear(fc1_size,1)\n",
        "\n",
        "    # not following the EEGNET here, just trying to get a simple one\n",
        "    #self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "\n",
        "    # maxpool_output_size is the total amount of data coming out of that\n",
        "    # layer.  Explain why the line of code below computes this quantity.\n",
        "    #self.maxpool_output_size = int(fc1_size * (data_dims[1] / 2) * (data_dims[2] / 2)) #div 2 b/c of either kern or stride\n",
        "\n",
        "    # Add on a fully connected layer (like in our MLP)\n",
        "    # fc stands for fully connected\n",
        "    #self.fc1 = nn.Linear(self.maxpool_output_size, fc1_size)\n",
        "\n",
        "    \n",
        "\n",
        "    # Convert our fully connected layer into outputs that we can compare to the result\n",
        "    #self.fc2 = nn.Linear(fc1_size, nb_classes)\n",
        "\n",
        "  # The forward function in the class defines the operations performed on a given input to the model\n",
        "  # and returns the output of the model\n",
        "  def forward(self, x):\n",
        "    # BLOCK 1\n",
        "    x = self.convtime(x)\n",
        "    x = self.batchnorm1(x)\n",
        "    #x = self.pool(x)\n",
        "    x = self.convdepth(x)\n",
        "    x = self.batchnorm2(x)\n",
        "    x = self.activation_func(x)\n",
        "    x = self.avgpool1(x)\n",
        "    x = nn.functional.dropout2d(x, .5) # Figure out how to put variables in here later\n",
        "    #BLOCK 2\n",
        "    x = self.convsep(x)\n",
        "    #might need another conv step here to get pointwise right, also dims are shrinking so need to fix padding\n",
        "    x = self.batchnorm3(x)\n",
        "    x = self.activation_func(x)\n",
        "    x = self.avgpool2(x)\n",
        "    x = nn.functional.dropout2d(x, .5) # Figure out how to put variables in here later\n",
        "\n",
        "    # Missing pointwise or separable convolution and several steps here, but just trying to get to linear while learning\n",
        "\n",
        "    x = x.view(-1,37*96) #figure out the other dimension based on batch_size and the size of x; this should flatten the matrix\n",
        "    x = self.fc1(x)\n",
        "    #x = torch.sigmoid(x)\n",
        "    #x = self.activation_func(x)\n",
        "    x = self.fc2(x)\n",
        "    x = torch.sigmoid(x)\n",
        "    #x = torch.nn.Softmax(x) # This might only work if you go to the two classes, since it needs numbers to sum to 1\n",
        "    return x\n",
        "\n",
        "  # The loss function (which we chose to include as a method of the class, but doesn't need to be)\n",
        "  # returns the loss and optimizer used by the model\n",
        "  def get_loss(self, learning_rate):\n",
        "    # Loss function\n",
        "    #loss = nn.CrossEntropyLoss()\n",
        "    # or you can use binary cross entropy\n",
        "    loss = torch.nn.BCELoss()\n",
        "    # Optimizer, self.parameters() returns all the Pytorch operations that are attributes of the class\n",
        "    optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "    return loss, optimizer\n",
        "\n",
        "\n",
        "def visualize_network(net):\n",
        "    # Visualize the architecture of the model\n",
        "    # We need to give the net a fake input for this library to visualize the architecture\n",
        "    \n",
        "    fake_input = Variable(torch.zeros((10,data_dims[0], data_dims[1], data_dims[2]))).to(device)\n",
        "    outputs = net(fake_input)\n",
        "    # Plot the DAG (Directed Acyclic Graph) of the model\n",
        "    return make_dot(outputs, dict(net.named_parameters()))\n",
        "\n",
        "\n",
        "# Initialize the model, loss, and optimization function\n",
        "net = EEGNet_SSVEP()\n",
        "# This tells our model to send all of the tensors and operations to the GPU (or keep them at the CPU if we're not using GPU)\n",
        "net.to(device)\n",
        "\n",
        "visualize_network(net)\n",
        "summary(net,data_dims)\n",
        "\n",
        "def evaluate(model, X, Y, batch_size, params = [\"acc\"]):\n",
        "    results = []\n",
        "    \n",
        "    predicted = []\n",
        "    \n",
        "    for i in range(int(len(X)/batch_size)): # not sure if this should be changed\n",
        "        s = i*batch_size\n",
        "        e = i*batch_size+batch_size\n",
        "        \n",
        "        inputs = Variable(torch.from_numpy(X[s:e]).cuda(0))\n",
        "        pred = model(inputs)\n",
        "        \n",
        "        predicted.append(pred.data.cpu().numpy())\n",
        "        \n",
        "        \n",
        "    inputs = Variable(torch.from_numpy(X).cuda(0))\n",
        "    predicted = model(inputs)\n",
        "    \n",
        "    predicted = predicted.data.cpu().numpy()\n",
        "    \n",
        "    for param in params:\n",
        "        if param == 'acc':\n",
        "            results.append(accuracy_score(Y, np.round(predicted)))\n",
        "        if param == \"auc\":\n",
        "            results.append(roc_auc_score(Y, predicted))\n",
        "        if param == \"recall\":\n",
        "            results.append(recall_score(Y, np.round(predicted)))\n",
        "        if param == \"precision\":\n",
        "            results.append(precision_score(Y, np.round(predicted)))\n",
        "        if param == \"fmeasure\":\n",
        "            precision = precision_score(Y, np.round(predicted))\n",
        "            recall = recall_score(Y, np.round(predicted))\n",
        "            results.append(2*precision*recall/ (precision+recall))\n",
        "    return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "both",
        "id": "0kDGhbJkZq0a",
        "colab": {}
      },
      "source": [
        "# Run the code to train and make the plots\n",
        "\n",
        "# Define training parameters\n",
        "batch_size = 25\n",
        "learning_rate = 1e-3\n",
        "n_epochs = 50\n",
        "\n",
        "\n",
        "# Get our data into the mini batch size that we defined\n",
        "#train_loader = torch.utils.data.DataLoader(X_train, batch_size=batch_size,\n",
        "#                                           sampler=train_sampler, num_workers=2)\n",
        "#print(train_loader)\n",
        "#test_loader = torch.utils.data.DataLoader(\n",
        "#    test_set, batch_size=128, sampler=test_sampler, num_workers=2)\n",
        "\n",
        "def train_model(net):\n",
        "    \"\"\" Train a the specified network.\n",
        "\n",
        "        Outputs a tuple with the following four elements\n",
        "        train_hist_x: the x-values (batch number) that the training set was \n",
        "            evaluated on.\n",
        "        train_loss_hist: the loss values for the training set corresponding to\n",
        "            the batch numbers returned in train_hist_x\n",
        "        val_hist_x: the x-values (batch number) that the test set was \n",
        "            evaluated on.\n",
        "        val_loss_hist: the loss values for the test set corresponding to\n",
        "            the batch numbers returned in test_hist_x\n",
        "    \"\"\" \n",
        "    loss, optimizer = net.get_loss(learning_rate)\n",
        "    # Define some parameters to keep track of metrics\n",
        "    print_every = 5\n",
        "    idx = 0\n",
        "    train_hist_x = []\n",
        "    train_loss_hist = []\n",
        "    val_hist_x = []\n",
        "    val_loss_hist = []\n",
        "\n",
        "    training_start_time = time.time()\n",
        "    # Loop for n_epochs\n",
        "    print('begin')\n",
        "    for epoch in range(n_epochs):\n",
        "      running_loss = 0.0\n",
        "      start_time = time.time()\n",
        "\n",
        "      # Loop through in batches on the train data\n",
        "      for i in range(int((len(X_train)/batch_size)-1)):\n",
        "        s = i*batch_size\n",
        "        e = i*batch_size+batch_size\n",
        "        inputs = torch.from_numpy(X_train[s:e])\n",
        "        labels = torch.from_numpy(np.array([y_train[s:e]]).T*1.0) #if need float possibly for other loss func\n",
        "        inputs, labels = Variable(inputs.to(device)), Variable(labels.to(device))  # wrap them in Variable\n",
        "        # In Pytorch, We need to always remember to set the optimizer gradients to 0 before we recompute the new gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        outputs = net(inputs)\n",
        "        # Compute the loss and find the loss with respect to each parameter of the model\n",
        "        loss_size = loss(outputs, labels)\n",
        "        loss_size.backward()\n",
        "        # Change each parameter with respect to the recently computed loss.\n",
        "        optimizer.step()\n",
        "        # Update statistics\n",
        "        running_loss += loss_size.data.item()\n",
        "        \n",
        "        # Print every Nth batch of an epoch\n",
        "        if (i % print_every) == print_every-1:\n",
        "            print(\"Epoch {}, Iteration {}\\t train_loss: {:.2f} took: {:.2f}s\".format(\n",
        "                epoch + 1, i+1,running_loss / print_every, time.time() - start_time))\n",
        "            # Reset running loss and time\n",
        "            train_loss_hist.append(running_loss / print_every)\n",
        "            train_hist_x.append(idx)\n",
        "            running_loss = 0.0\n",
        "            start_time = time.time()\n",
        "        idx += 1\n",
        "      # At the end of the epoch, do a pass on the test set\n",
        "      total_val_loss = 0\n",
        "      for vi in range(int((len(X_val)/(batch_size))-1)):\n",
        "          svi = vi*batch_size\n",
        "          evi = vi*batch_size+batch_size\n",
        "          valinputs = torch.from_numpy(X_val[svi:evi])\n",
        "          vallabels = torch.from_numpy(np.array([y_val[svi:evi]]).T*1.0) #if need float possibly for other loss func\n",
        "          # wrap them in Variable\n",
        "          valinputs, vallabels = Variable(valinputs.to(device)), Variable(vallabels.to(device))  \n",
        "          # Forward pass\n",
        "          val_outputs = net(valinputs)\n",
        "          val_loss_size = loss(val_outputs, vallabels)\n",
        "          total_val_loss += val_loss_size.data.item()\n",
        "      val_loss_hist.append(total_val_loss / len(X_val))\n",
        "      val_hist_x.append(idx)\n",
        "      print(\"Validation loss = {:.2f}\".format(\n",
        "          total_val_loss / len(y_val)))\n",
        "      \n",
        "    print(\"Training finished, took {:.2f}s\".format(\n",
        "        time.time() - training_start_time))\n",
        "    return train_hist_x, train_loss_hist, val_hist_x, val_loss_hist\n",
        "\n",
        "# RUN the TRAINING\n",
        "train_hist_x, train_loss_hist, val_hist_x, val_loss_hist = train_model(net)\n",
        "\n",
        "plt.plot(train_hist_x,train_loss_hist)\n",
        "plt.plot(val_hist_x,val_loss_hist)\n",
        "plt.legend(['train loss', 'validation loss'])\n",
        "plt.xlabel('Batch number')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AenGz_HGX3iy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(X_val.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqBo3YUSKC18",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plots of the conv kernels learned by the model\n",
        "plt.figure()\n",
        "plt.subplots(6, 4)\n",
        "for i in range(net.convtime.weight.shape[0]):\n",
        "    plt.subplot(6, 4, i+1)\n",
        "    kernel = net.convtime.weight[i].cpu().detach().numpy()\n",
        "    im = kernel.mean(axis=0)\n",
        "    plt.pcolor(im, cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.subplots(14, 6)\n",
        "for i in range(net.convdepth.weight.shape[0]):\n",
        "    plt.subplot(14, 6, i+1)\n",
        "    kernel = net.convdepth.weight[i].cpu().detach().numpy()\n",
        "    im = kernel.mean(axis=0)\n",
        "    plt.pcolor(im, cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.subplots(14, 6)\n",
        "for i in range(net.convsep.weight.shape[0]):\n",
        "    plt.subplot(14, 6, i+1)\n",
        "    kernel = net.convsep.weight[i].cpu().detach().numpy()\n",
        "    im = kernel.mean(axis=0)\n",
        "    plt.pcolor(im, cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}